{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9432914,"sourceType":"datasetVersion","datasetId":5731113},{"sourceId":9433943,"sourceType":"datasetVersion","datasetId":5731878}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Madhavan M\n#Sriram V\n#siddarth\n#auston\n\n\n\n#Data preprocessing is done as the ratio of no:yes is 5:1 and saved as cleaned dataset\n\nimport pandas as pd\n\n# Load the dataset from CSV\ndf = pd.read_csv('/kaggle/input/datathon/DATATHON_EVENT_DATASET.csv')\n\n# Drop the first 30,000 rows from the dataframe as it contains no\ndf_dropped = df.iloc[30000:].reset_index(drop=True)\n\n# Display the shape of the updated dataframe\nprint(\"Shape of dataframe after dropping 30,000 rows:\", df_dropped.shape)\n\n# Save the cleaned dataframe to a new CSV file\ndf_dropped.to_csv('cleaned_dataset.csv', index=False)\n\nprint(\"Cleaned dataset saved successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:59:29.605105Z","iopub.execute_input":"2024-09-19T09:59:29.605777Z","iopub.status.idle":"2024-09-19T09:59:30.007208Z","shell.execute_reply.started":"2024-09-19T09:59:29.605737Z","shell.execute_reply":"2024-09-19T09:59:30.006313Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Shape of dataframe after dropping 30,000 rows: (28150, 11)\nCleaned dataset saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import libraries\n#training and testing the model\nimport pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# Load the dataset from cleaned data CSV\ndf = pd.read_csv('/kaggle/input/cleaned-dataset/cleaned_dataset.csv')\n\n# ---- Data Cleaning ----\n# Use SimpleImputer for filling missing values based on the column type\n# Filling numerical columns with median and categorical with mode\nnumerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\ncategorical_cols = df.select_dtypes(include=['object']).columns\n\n# Impute missing values\ndf[numerical_cols] = SimpleImputer(strategy='median').fit_transform(df[numerical_cols])\ndf[categorical_cols] = SimpleImputer(strategy='most_frequent').fit_transform(df[categorical_cols])\n\n# ---- Label Conversion ----\n# Convert 'Yes'/'No' in 'Fraud' column to 1 and 0\ndf['Fraud'] = df['Fraud'].map({'Yes': 1, 'No': 0})\n\n# ---- Define Features (X) and Target (y) ----\nX = df.drop(columns=['Fraud'])  # Features (drop the target column)\ny = df['Fraud']  # Target (fraud label)\n\n# ---- Handling High Cardinality Columns ----\n# Label encode high cardinality columns (like Origin_ID and Destination_ID)\nlabel_encoder = LabelEncoder()\nX['Origin_ID'] = label_encoder.fit_transform(X['Origin_ID'])\nX['Destination_ID'] = label_encoder.fit_transform(X['Destination_ID'])\n\n# ---- One-Hot Encoding for Categorical Columns ----\n# One-Hot Encode 'Transaction_Type' and 'Expected_Fraud' columns\ncategorical_cols = ['Transaction_Type', 'Expected_Fraud']\npreprocessor = ColumnTransformer(\n    transformers=[('cat', OneHotEncoder(drop='first'), categorical_cols)],  # One-Hot Encoding\n    remainder='passthrough'  # Leave other columns as is (numerical ones)\n)\n\n# ---- Create the Pipeline ----\n# The pipeline includes preprocessing steps (encoding + scaling) and the model (RandomForestClassifier)\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),  # Preprocessing (One-Hot Encoding and pass-through other features)\n    ('scaler', StandardScaler()),  # Scale numerical features\n    ('classifier', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42))  # Random Forest with parameters\n])\n\n# ---- Splitting the Data ----\n# Split the data into training and testing sets (60% train, 40% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n\n# ---- Train the Model ----\n# Train the model using the training set\npipeline.fit(X_train, y_train)\n\n# ---- Predictions and Evaluation ----\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Calculate F1 Score, Confusion Matrix, and Accuracy\nf1 = f1_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the results\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint(f'F1-Score: {f1}')\nprint('Confusion Matrix:')\nprint(cm)\n\n# ---- Save the Trained Model ----\n# Save the model pipeline using joblib\njoblib.dump(pipeline, 'fraud_detection_pipeline.pkl')\nprint(\"Model saved successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:05:39.032371Z","iopub.execute_input":"2024-09-19T10:05:39.032986Z","iopub.status.idle":"2024-09-19T10:05:42.935814Z","shell.execute_reply.started":"2024-09-19T10:05:39.032935Z","shell.execute_reply":"2024-09-19T10:05:42.934843Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Accuracy: 98.13%\nF1-Score: 0.967451952882827\nConfusion Matrix:\n[[7929   71]\n [ 139 3121]]\nModel saved successfully.\n","output_type":"stream"}]}]}