{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9432914,"sourceType":"datasetVersion","datasetId":5731113}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Madhavan M\n#Sriram V\n#Sid\n#Auston\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix\n\n# Load the data\ndf = pd.read_csv('/kaggle/input/datathon/DATATHON_EVENT_DATASET.csv')\n\n# Fill missing values (if any)\ndf.fillna(method='ffill', inplace=True)\n\n# Convert 'Yes'/'No' in 'Fraud' column to 1 and 0\ndf['Fraud'] = df['Fraud'].map({'Yes': 1, 'No': 0})\n\n# Define feature columns and target\nX = df.drop(columns=['Fraud'])  # Features (drop the target column)\ny = df['Fraud']  # Target\n\n# Label encode columns with high cardinality (like Origin_ID and Destination_ID)\nlabel_encoder = LabelEncoder()\n\nX['Origin_ID'] = label_encoder.fit_transform(X['Origin_ID'])\nX['Destination_ID'] = label_encoder.fit_transform(X['Destination_ID'])\n\n# Categorical columns with few unique values, we can apply One-Hot Encoding (e.g., Transaction_Type, Expected_Fraud)\ncategorical_cols = ['Transaction_Type', 'Expected_Fraud']\n\n# One-Hot Encoding for categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), categorical_cols)],\n    remainder='passthrough'\n)\n\n# Create pipeline with preprocessing and classifier\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),  # Standardize numerical features\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Fit the pipeline model\npipeline.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipeline.predict(X_test)\n\nf1 = f1_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the results\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint(f'F1-Score: {f1}')\nprint('Confusion Matrix:')\nprint(cm)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:18:52.664891Z","iopub.execute_input":"2024-09-19T08:18:52.665544Z","iopub.status.idle":"2024-09-19T08:18:59.141985Z","shell.execute_reply.started":"2024-09-19T08:18:52.665502Z","shell.execute_reply":"2024-09-19T08:18:59.141008Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1138372215.py:18: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df.fillna(method='ffill', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 98.91%\nF1-Score: 0.9604976671850699\nConfusion Matrix:\n[[9959   41]\n [  86 1544]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Team Members:\n# Madhavan M\n# Sriram V\n# Sid\n# Auston\n\nimport pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nimport numpy as np\n\n# ---------------------- PART 1: Training and Saving the Model ---------------------- #\n\n# Load the data\ndf = pd.read_csv('/kaggle/input/datathon/DATATHON_EVENT_DATASET.csv')\n\n# Fill missing values (if any)\ndf.fillna(method='ffill', inplace=True)\n\n# Convert 'Yes'/'No' in 'Fraud' column to 1 and 0\ndf['Fraud'] = df['Fraud'].map({'Yes': 1, 'No': 0})\n\n# Define feature columns and target\nX = df.drop(columns=['Fraud'])  # Features (drop the target column)\ny = df['Fraud']  # Target\n\n# Label encode columns with high cardinality (like Origin_ID and Destination_ID)\nlabel_encoder = LabelEncoder()\n\nX['Origin_ID'] = label_encoder.fit_transform(X['Origin_ID'])\nX['Destination_ID'] = label_encoder.fit_transform(X['Destination_ID'])\n\n# Categorical columns with few unique values, we can apply One-Hot Encoding (e.g., Transaction_Type, Expected_Fraud)\ncategorical_cols = ['Transaction_Type', 'Expected_Fraud']\n\n# One-Hot Encoding for categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), categorical_cols)],\n    remainder='passthrough'\n)\n\n# Create pipeline with preprocessing and classifier\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),  # Standardize numerical features\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Fit the pipeline model\npipeline.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipeline.predict(X_test)\n\n# Calculate F1 Score, Confusion Matrix, and Accuracy\nf1 = f1_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the results\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint(f'F1-Score: {f1}')\nprint('Confusion Matrix:')\nprint(cm)\n\n# Save the entire pipeline (preprocessor + model)\njoblib.dump(pipeline, 'fraud_detection_pipeline.pkl')\nprint(\"Model saved successfully.\")\n\n# ---------------------- PART 2: Loading the Model and Classifying New Data ---------------------- #\n\nimport pandas as pd\nimport joblib\nimport numpy as np\n\n# Load the saved pipeline\npipeline = joblib.load('fraud_detection_pipeline.pkl')\nprint(\"Model loaded successfully.\")\n\n# Function to handle unseen labels in the custom data\ndef label_encode_custom(label_encoder, series):\n    existing_classes = label_encoder.classes_\n    new_classes = series[~series.isin(existing_classes)]\n\n    # Add new classes to the LabelEncoder\n    if len(new_classes) > 0:\n        label_encoder.classes_ = np.append(existing_classes, new_classes)\n    \n    return label_encoder.transform(series)\n\n# Sample data for a new transaction (make sure it matches the structure of the training data)\ndata = {\n    'Time': [123],  # the time in real-world units\n    'Transaction_Type': ['TRANSFER'],  # type of transaction\n    'Amount': [5000.00],  # transaction amount\n    'Origin_ID': ['C123456789'],  # customer who initiated the transaction\n    'Initial_Origin_Balance': [10000.00],  # balance before transaction\n    'Final_Origin_Balance': [5000.00],  # balance after transaction\n    'Destination_ID': ['C987654321'],  # recipient of the transaction\n    'Initial_Destination_Balance': [2000.00],  # recipient's balance before transaction\n    'Final_Destination_Balance': [7000.00],  # recipient's balance after transaction\n    'Expected_Fraud': ['No']  # if the transaction is expected to be fraudulent based on business rules\n}\n\n# Convert dictionary to DataFrame\nnew_transaction = pd.DataFrame(data)\n\n# Apply the same preprocessing as done during model training\n# Label Encoding for IDs, handling unseen labels\nnew_transaction['Origin_ID'] = label_encode_custom(label_encoder, new_transaction['Origin_ID'])\nnew_transaction['Destination_ID'] = label_encode_custom(label_encoder, new_transaction['Destination_ID'])\n\n# Ensure that the columns of the new transaction match the original training data structure\nnew_transaction = new_transaction.reindex(columns=X_train.columns, fill_value=0)\n\n# One-Hot Encoding for categorical features\n# Assuming 'preprocessor' is your preprocessing pipeline that was fitted with your training data\nnew_transaction_processed = pipeline.named_steps['preprocessor'].transform(new_transaction)\n\n# Predict using your trained pipeline\npredicted_class = pipeline.predict(new_transaction_processed)\n\n# Output whether the transaction is legit (0) or fraudulent (1)\nif predicted_class[0] == 0:\n    print(\"The transaction is legitimate.\")\nelse:\n    print(\"The transaction is fraudulent.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:18:59.143768Z","iopub.execute_input":"2024-09-19T08:18:59.144112Z","iopub.status.idle":"2024-09-19T08:19:05.925432Z","shell.execute_reply.started":"2024-09-19T08:18:59.144078Z","shell.execute_reply":"2024-09-19T08:19:05.924127Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4289231195.py:23: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df.fillna(method='ffill', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 98.91%\nF1-Score: 0.9604976671850699\nConfusion Matrix:\n[[9959   41]\n [  86 1544]]\nModel saved successfully.\nModel loaded successfully.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m new_transaction_processed \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(new_transaction)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Predict using your trained pipeline\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_transaction_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Output whether the transaction is legit (0) or fraudulent (1)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_class[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:480\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    478\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:798\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(\n\u001b[1;32m    801\u001b[0m     X,\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    805\u001b[0m     column_as_strings\u001b[38;5;241m=\u001b[39mfit_dataframe_and_transform_dataframe,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: X has 15 features, but ColumnTransformer is expecting 10 features as input."],"ename":"ValueError","evalue":"X has 15 features, but ColumnTransformer is expecting 10 features as input.","output_type":"error"}]}]}